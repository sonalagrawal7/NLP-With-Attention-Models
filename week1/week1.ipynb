{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "video/1_Introduction.mp4\n",
    "\n",
    "<video controls src=\"video/1_Introduction.mp4\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Reading\n",
    "\n",
    "Connect with your mentors and fellow learners on Slack!\n",
    "Hi! \n",
    "\n",
    "We have 2 communication platforms:\n",
    "\n",
    "1. Coursera Discussion Forums\n",
    "Here, you will find dedicated mentors who provide academic support for this course. They do this by answering questions, explaining concepts, giving feedback, and sharing resources in the course forums. \n",
    "\n",
    "* Please feel free to ask questions and interact with mentors on the forums.\n",
    "* As a reminder, the Coursera Honor Code states that you should not share your solutions to homeworks, quizzes, etc. So, please do not post any code on the forums.\n",
    "* Please search for your question in forums before writing a new one. It may be answered already.\n",
    "\n",
    "2. Slack\n",
    "The slack channel is a community for learners to meet other learners from around the world, ask questions, discuss ideas and work together.\n",
    "You can pair up with learners that have similar interests. For example paper discussions, kaggle competitions, hackathons, share opportunities etc\n",
    "\n",
    "In addition, you will learn about opportunities to work more directly with deeplearning.ai as mentors. \n",
    "\n",
    "Please join the Slack workspace by going to the following link deeplearningai-nlp.slack.com\n",
    "\n",
    "\n",
    "This link to join slack was last updated on October 28, 10:00M PST.  If the link has expired, please reply to this [discussion forum post](https://www.coursera.org/learn/attention-models-in-nlp/discussions/weeks/1/threads/2nwrQfxdEeq_pBKnNBIIuQ) and we will update the invitation link.\n",
    "\n",
    "\n",
    "**NOTE: Mentors will be answering questions on Coursera forums so that all questions and answers are in the same place.  So, please ask questions on the forums.**\n",
    "\n",
    "#### The mentors for this specialization are:\n",
    "Sven Chilton\n",
    "\n",
    "Luis Alaniz\n",
    "\n",
    "Anel Nurkayeva\n",
    "\n",
    "Gordon Robinson\n",
    "\n",
    "Geoff Ladwig\n",
    "\n",
    "Manish Jain\n",
    "\n",
    "Mo Rebaie \n",
    "\n",
    "Deyber Valencia\n",
    "\n",
    "Rishit Dholakia\n",
    "\n",
    "Ahti Kitsik\n",
    "\n",
    "Anmol Gupta\n",
    "\n",
    "Luis Gerardo Ayala \n",
    "\n",
    "Parth Agrawal\n",
    "\n",
    "Shovan Kumar Paul\n",
    "\n",
    "Didi Milikina \n",
    "\n",
    "Shashank Pathak\n",
    "\n",
    "Manfred Vogel\n",
    "\n",
    "Mahmoud Soliman\n",
    "\n",
    "Chak Wong\n",
    "\n",
    "Petra Vanickova\n",
    "\n",
    "Ilir Kondo\n",
    "\n",
    "Vadim Popov\n",
    "\n",
    "Amin Anvari\n",
    "\n",
    "Giuseppe Fasanella\n",
    "\n",
    "Vladimir Cucu\n",
    "\n",
    "Carlos R Lacerda\n",
    "\n",
    "Sachin Gopal Wani\n",
    "\n",
    "Loucas Loumakos\n",
    "\n",
    "Bernard Vanhopplijnus\n",
    "\n",
    "Aakash Agarwal\n",
    "\n",
    "Giuseppe De Ruvo\n",
    "\n",
    "Pankaj Kang\n",
    "\n",
    "Valeri Voev\n",
    "\n",
    "Arka Mitra\n",
    "\n",
    "Nikesh Bajaj\n",
    "\n",
    "Alessandro Tiberi\n",
    "\n",
    "Rohit Sharma\n",
    "\n",
    "### The QA Manager will be in Slack to collect and prioritize updates based on your feedback.\n",
    "Arturo Polanco\n",
    "\n",
    "### Learning Technologists will also be in Slack, and will be updating the course content based on your feedback.\n",
    "Chris Favila\n",
    "\n",
    "Mubsi Khawar\n",
    "\n",
    "Parth Agrawal\n",
    "\n",
    "We hope to see you there!\n",
    "\n",
    "-The DeepLearning.AI team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2seq\n",
    "\n",
    "video/3_Seq2seq.mp4\n",
    "\n",
    "<video controls src=\"video/3_Seq2seq.mp4\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why are longer sequences problematic for a traditional seq2seq model?\n",
    "\n",
    "\n",
    "Because seq2seq relies on a fixed-length memory.\n",
    "\n",
    "\n",
    "\n",
    "Correct\n",
    "That’s right! This means when the input is larger than the memory, an information bottleneck can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alignment\n",
    "\n",
    "video/4_Alignment.mp4\n",
    "\n",
    "<video controls src=\"video/4_Alignment.mp4\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Background on seq2seq\n",
    "Recurrent models typically take in a sequence in the order it is written and use that to output a sequence. Each element in the sequence is associated with its step in computation time tt. (i.e. if a word is in the third element, it will be computed at $t_3$. These models generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t−1}$ and the input for position t. \n",
    "\n",
    "The sequential nature of models you learned in the previous course (RNNs, LSTMs, GRUs) does not allow for parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. In other words, if you rely on sequences and you need to know the beginning of a text before being able to compute something about the ending of it, then you can not use parallel computing. You would have to wait until the initial computations are complete. This is not good, because if your text is too long, then 1) it will take a long time for you to process it and 2) you will lose a good amount of information mentioned earlier in the text as you approach the end.  \n",
    "\n",
    "Therefore, attention mechanisms have become critical  for sequence modeling in various tasks, allowing modeling of dependencies without caring too much about their distance in the input or output sequences. \n",
    "\n",
    "In this course, you will learn about these attention mechanisms and see how they are implemented. Welcome to Course 4! \n",
    "\n",
    "PS: Here's an article from The Atlantic that discusses the famous JFK speech containing the words \"Ich bin ein Berliner,\" the example you saw in the Alignment video.\n",
    "\n",
    "https://www.theatlantic.com/magazine/archive/2013/08/the-real-meaning-of-ich-bin-ein-berliner/309500/ (Putnam, 2013) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional): The Real Meaning of Ich Bin ein Berliner\n",
    "\n",
    "Here's an article from The Atlantic that discusses the famous JFK speech containing the words \"Ich bin ein Berliner,\" the example you saw in the Alignment video.\n",
    "\n",
    "https://www.theatlantic.com/magazine/archive/2013/08/the-real-meaning-of-ich-bin-ein-berliner/309500/ (Putnam, 2013) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attention\n",
    "\n",
    "video/7_Attention.mp4\n",
    "\n",
    "<video controls src=\"video/7_Attention.mp4\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "The previous video covered one of the most important concepts covered in this course. If you did not get what I was saying right away, don't worry about it. This reading will explain everything I mentioned in detail.\n",
    "\n",
    "The attention mechanism uses encoded representations of both the input or the encoder hidden states and the outputs or the decoder hidden states. The keys and values are pairs. Both of dimension NN, where NN is the input sequence length and comes from the encoder hidden states.\n",
    "\n",
    "<img src=\"Images/tVlC8-qdTRWZQvPqnV0VKg_c3fce0ddebb94d8eb15ca2cc19348b8e_Screen-Shot-2020-11-05-at-11.26.53-AM.png\">\n",
    "\n",
    "Keys and values have their own respective matrices, but the matrices have the same shape and are often the same. While the queries come from the decoder hidden states. One way you can think of it is as follows. Imagine that you are translating English into German. You can represent the word embeddings in the English language as keys and values. The queries will then be the German equivalent. You can then calculate the dot product between the query and the key. Note that similar vectors have higher dot products and non-similar vectors will have lower dot products. The intuition here is that you want to identify the corresponding words in the queries that are similar to the keys. This would allow your model to \"look\" or focus on the right place when translating each word. We then run a softmax\n",
    "\n",
    "$softmax(QK^T)$                                                \n",
    "equ(1)\n",
    "\n",
    "\n",
    "That allows us to get a distribution of numbers between 0 and 1.  One more step is required. We then would multiply the output by VV. Remember VV in this example was the same as our keys, corresponding to the English word embeddings. Hence the equation becomes\n",
    "\n",
    "$softmax(QK^T)V$                                                \n",
    "equ(2)\n",
    "\n",
    "This tells us how much of each word, or which combination of words we will be feeding into our decoder to predict the next German word. This is called scale dot product attention. Note we add a normalization constant to it later, but you do not have to worry about that right now. \n",
    "\n",
    "Here's an example that shows where the model is looking when translating between English and German, as you'll be doing in your assignments. The attention model should still be able to find a connection between the two words. \n",
    "\n",
    "   \n",
    "<img src=\"Images/RWXquYJfSYal6rmCX_mG_Q_0e0ba0a45e3d4368bfe8a3c7d3872145_Screen-Shot-2020-11-05-at-12.51.16-PM.png\">\n",
    "\n",
    "In the matrix, the lighter square shows where the model is actually looking when making the translation of that word. This mapping should not necessarily be one to one. The lighting just tells you to what extent is each word contributing to the input that will be fed into the decoder. As you can see several words can contribute to translating another word, depending on the weights (output) of the softmax that will be used to create the new input. \n",
    "\n",
    "\n",
    "<img src=\"Images/luf7KWwaT2Sn-ylsGg9kkA_dbebeeb223ca43b3bb05baad8795230b_Screen-Shot-2020-11-05-at-12.55.01-PM.png\">\n",
    "\n",
    "So attention is a layer of calculations that let your model focus on the most important parts of the sequence for each step. Queries, values, and keys are representations of the encoder and decoder hidden states. And they're used to retrieve information inside the attention layer by calculating the similarity between the decoder queries and the encoder key- value pairs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"Images/\">\n",
    "\n",
    "<video controls src=\"video/\"/>\n",
    "<video controls src=\"video/\"/>\n",
    "<video controls src=\"video/\"/>\n",
    "<video controls src=\"video/\"/>\n",
    "<video controls src=\"video/\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
